## ML lec04 : Multi-variable Linear Regression
### Recap
- Hypothesis : $H(x) = Wx + b$
  - 주어진 $x$에 대한 가설
- Cost function(=Loss function) : $cost(W,b) = \frac{1}{m} \Sigma_{i=1}^{m}(H(x^{(i)} - y^{(i)})^2$
  - 실제 값과 예측 값의 차이 제곱 합의 평균
- Gradient descent algorithm
  - cost가 최소화 되는 값을 찾기 위해 경사면을 따라 내겨라는 알고리즘

### Linear Regression using three inputs($x1$, $x2$, $x3$)
- [ ] (TBD) 3:45~
- input이 많아질수록 수식 표현이 어려움 → 잘 표현하는 방법 : Matrix

### Hypothesis using matrix
- Matrix multiplication
  - Dot product
- [ ] (TBD) 6:17~ 식 표현
- $H(X) = XW$
  - matrix를 쓸때는 보통 $X$를 앞에 써서 표현

### Many $x$ instances
- [ ] instance?
- 연산을 더욱 효율적으로 표현하는 방법은? →
- [ ] (TBD) 10:05~ 식 표현

### $W$ 크기 결정하기 : matrix 연산의 shape
- $X$ : $[5,3]$
- $W$ : $[3,1]$
- $H(X)$ : $[5,1]$
- 보통 $X$와 출력값$H(X)$의 형태는 주어진다
  - $X$ : [#instance(=n), #variable]
  - $H(X)$ : [#instance(=n), #y]
- 따라서 $W$의 크기는 우리가 결정해야 한다 → $X$와 $H(X)$를 이용해서 쉽게 결정할 수 있음

### $WX$ vs $XW$
- Lecture(theory) : $H(x) = Wx + b$
- Implementation(TensorFlow) : $H(X) = XW$
- 수학적 의미는 똑같다
